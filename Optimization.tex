% !TEX root = Main.tex
\section{Optimization}
To show that an objective is not convex, compute Hessian and show that it's not p.s.d. (det(H) < 0).

\subsection*{Alternating Least Squares}
Optimising L(U,V) is not convex, but keeping one of them fixed and optimising the other is.
$f(U,v_j)=\sum_{(i,j)\in I} (a_{i,j} - \langle u_i, v_j \rangle)^2 + \lambda \sum_i \lVert\mathbf{u_i}\rVert^2 +(...)$\\
$f(u_i,V)=\sum_{(i,j)\in I} (a_{i,j} - \langle u_i, v_j \rangle)^2 + (...) + \lambda \sum_j \lVert\mathbf{v_j}\rVert^2$\\
$u_i = (\sum_{i,j \in \mathcal{I}} v_jv_j^T + \lambda I_k)^{-1} \sum_{i,j \in \mathcal{I}} a_{ij}v_j$
Derived by setting partial derivative w.r.t $u_i$ to 0.

\subsection*{Gradient Descent (or Deepest Descent)}
\textbf{Gradient}: $\nabla f(\mathbf{x}) := \left( \frac{\partial f(\mathbf{x})}{\partial \mathbf{x}_1}, \ldots, \frac{\partial f(\mathbf{x})}{\partial \mathbf{x}_D} \right)^\top$

\begin{inparaenum}[\color{red}1.]
	\item init: $\mathbf{x}^{(0)} \in \mathbb{R}^D$
	\item for $t = 0 \ \text{to} \ \mathit{maxIter}$:
	\item $\mathbf{x}^{(t+1)} = \mathbf{x}^{(t)} - \gamma \nabla_x f(\mathbf{x}^{(t)})$, usually $\gamma \approx \frac{1}{t}$
\end{inparaenum}

\subsection*{Stochastic Gradient Descent (SGD)}
Assume \textbf{Additive Objective}: $f(x) = \frac{1}{N}\sum_{n=1}^{N}f_n(x)$\\

\begin{inparaenum}[\color{red}1.]
	\item init: $\mathbf{x}^{(0)} \in \mathbb{R}^D$
	\item for $t = 0 \ \text{to} \ \mathit{maxIter}$:
	\item sample $n \in_{u.a.r.} \{1, \ldots, N\}$
	\item $\mathbf{x}^{(t+1)} = \mathbf{x}^{(t)} - \gamma \nabla_x f_n(\mathbf{x}^{(t)})$, typically  $\gamma \approx \frac{1}{t}$.
\end{inparaenum}

\subsection*{Projected Gradient Descent (Constrained Opt.)}
minimize $f(x)$, $x \in Q$ (constraint).\\
\textbf{Project} $x$ onto $Q$: $P_Q(\mathbf{x}) = \argmin_{y \in Q} \|\mathbf{y} - \mathbf{x}\|$,\\
\textbf{Update}: $\mathbf{x}^{(t+1)} = P_Q[\mathbf{x}^{(t)} - \gamma \nabla f(\mathbf{x}^{(t)})]$,\\
$\mathbf{x}^{(t+1)}$ is unique if $Q$ convex.

\subsection*{Lagrangian Multipliers}
Minimize  $f(\mathbf{x})$ s.t. $g_i(\mathbf{x}) \leq 0,\ i = 1, .., m$ (\textbf{inequality constr.}) and $h_i(\mathbf{x}) = \mathbf{a}_i^\top \mathbf{x} - b_i = 0,\ i = 1, .., p$ (\textbf{equality constraint}). 
\textbf{Lagrangian:} $L(\mathbf{x}, \boldsymbol{\lambda}, \boldsymbol{\nu}) := f(\mathbf{x}) + \sum_{i=1}^m \lambda_i g_i(\mathbf{x}) + \sum_{i=1}^p \nu_i h_i(\mathbf{x})$\\

\subsection*{Convex Optimization}
Def.: $\{(x,t)|x \in dom f, f(x) \leq t\}$, $f : \mathbb{R}^D \rightarrow \mathbb{R}$ is convex, if $dom\ f$ is a convex set, and if $\forall \mathbf{x}, \mathbf{y} \in dom\ f$, and for $0 \leq \alpha \leq 1$: $f(\alpha \mathbf{x} + (1 - \alpha)\mathbf{y}) \leq \alpha f(\mathbf{x}) + (1-\alpha)f(\mathbf{y})$. local=global min, \textbf{Convergence}: $f(\mathbf{x}^{(t)}) - f(\mathbf{x}^*) \le \frac{c}{t}$.
\textbf{Subgradient} $g \in \mathbb{R}^D$ of $f$ at $\mathbf{x}$: $f(\mathbf{y}) \geq f(\mathbf{x}) + g^\top(\mathbf{y}-\mathbf{x}) \ \forall \mathbf{y}$

\subsection*{Convex Relaxation}
Replace non-convex rank constraints by convex norm constraints.
$rank(B) \geq ||B||_*$. Repeat: $B^* = shrink_\tau(A) = arg min_B(\frac{1}{2}||A - B||_F^2 + \tau ||B||_*)$, with $B^*=UD_\tau V^T, D_\tau = diag(max(0,\sigma_i - \tau))$. Then $B_{t+1} = B_t + \eta_t \Pi(A - shrink_\tau(B_t))$, where $\Pi$ cuts off $x < 0$. Exact reconstruction of rank k matrix $A^*$ w.h.p, if it is strongly incoherent, if $\mathcal{I} \in O(n)$, spread of $\sigma_i$ large enough. 
